{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model = \"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=MDBG2MOp4Go\", add_video_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Failed to fetch transcript: Could not determine the video ID for the URL \"https://www.youtube.com/watch?v=MDBG2MOp\".'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    loader = YoutubeLoader.from_youtube_url(\n",
    "        \"https://www.youtube.com/watch?v=MDBG2MOp\", add_video_info=False\n",
    "    )\n",
    "    response = loader.load()    \n",
    "    print({\"transcript\": response[0].page_content})\n",
    "\n",
    "except Exception as e:\n",
    "    print({\"error\": f\"Failed to fetch transcript: {str(e)}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello all, my name is Krishna and welcome to my YouTube channel. So guys, welcome to this short crash course on MCP that is model context protocol. In this specific video, we are going to cover many topics. I will be talking about the agenda but everything that has come till now related to MCP I will be covering in this specific video from explaining you the concepts to building MCP servers from scratch to integrate MCP servers from various companies along with that uh we will also try to see that how we can use uh MCP servers in order to build our agentic AI application uh all the specific things we will be covering up uh please make sure that you watch this video till the end. Uh this will be a little long video but uh if you are able to cover almost each and everything in this particular video, you will be able to get a very good understanding about MCP itself. So quickly uh let me go ahead and share my screen. So uh I hope everybody has probably seen this particular um website for the model context protocol which was uh which was brought by anthropic and uh let me talk about the agenda what all things we are basically going to cover. Okay. Um already I have made a very dedicated video to make you understand about what exactly is MCP and how does MCP works right. So again in this agenda we will cover what is MCP. Again a small revision we'll try to do it. Then we will build MCP server from scratch. Then how we can run this server from an MCP inspector cloudy uh cloudy desktop and cursor ID. You can also use VS code ID. It is up to you. Then uh we will try to integrate some thirdparty servers directly into this into cloud or cursor. We'll see that. Then we'll try to integrate MCB server with client also. Right? Client basically means it can be a Streamlate app. It can be some other app you know uh it can also be developed by fast API right I will be showing you that specific examples then I will show you that how you can integrate MCP servers because at the end of the day uh all the services that we are using in the form of MCP server we have to provide context to the LLM models right so we'll try to show you how you can integrate with LMS uh here the main aim will be that how we can get the context from those services and we can create specific agents and we can implement solutions and finally we are going to go ahead and uh also implement the docker setup uh which will be very important for the deployment purpose because once we do this you can do do the deployment in EC2 instance in AWS Azure wherever you want okay so the first topic uh uh is about what is MCP and before I go ahead I quickly want to announce about this amazing course that we are coming up that is 2.0 O live aentic AI and uh MC uh and genai with MCP right so this is like if you just go ahead and click the link over here and enroll now here you'll be able to see all the information right so this is my first MCP batch along with the agent and second aent and genai batch uh the start date is May 10th 2025 every Saturday and Sunday we are just going to have 3 hours of session every uh Saturday and Sunday and our main aim is basically to cover each and everything that you require in order to learn agentic AI, a generative AI and all right so you can just go ahead and check out all the information will be given in the description of this particular video okay now quickly let me go back again to the agenda and first of all let's discuss uh as I have already told you what exactly is MCP uh but just a quick revision here you'll be able to see that um this is a general architecture for a MCP right so here you have something like host with MCP client it can be cloudy ID tools and all and then this will be communicating uh this uh this MCP client will be communicating with various servers like MCP server A B C they use MCP protocol and this MCP server uh may be interacting to some kind of local data source local data so uh it can be um other than data source it can also be APIs from the same companies it can be remote APIs also right or it can be remote database also through the internet okay So um if you are developing if you're learning generative AI specifically from me you know before how we used to have let's say if this is my um LLM right then we used to basically use various services let's say these are my se separate services we used to use it right now when we used to use the specific services here one thing you could observe right let's say if this is my LLM or AI assistant right this AI assistant will not be able to answer anything that is related to the current things right let's say if I want some information what is the recent news right so this AI assistant or the LLM will not be able to probably give you that specific answer because this is not connected to the internet so what you used to do is that you were connecting this with external service providers right now this service provider can be like Tavi API with the help of Tavly API you are actually connecting to Tavi you can connect to Doug go search right duck go search duck dug go search and you were basically communicating and getting the recent information let's say if I write hey what is the recent news in AI then what this communication used to happen over here right because obviously will not be able to answer this now to connect to this right you were writing all the code over here right obviously uh you had some of the packages that were available even in lang chain or different libraries you had these packages and with this specific packages you needed to write the entire wrapper code, right? Write write the wrapper code and with the help of this specific code, you were basically communicating with this particular uh library with this particular services, right? Now, what used to happen is that regularly this changes were happening over here. Now, because of these changes, you also had to update in the wrapper code. That was the scenario that used to happen. Not in every packages but yes as you keep on getting updated as the services get keeps on getting updated you also writing the code over here and making that update over here itself right now what MCP basically does and again I'll tell you over here this is just one services this can be my another services right this can be my another services let's say I want to search for some kind of research paper I was using some package called as rsafe if I want to probably do a Wikipedia search I was using this right Wikipedia search I was using this services. But here you could see that we were writing more code over here and we are managing even if there is any updates in the services we also managing this specific thing right this was one of the main challenges before when we are generating things now what happens because of MCP now because of MCP now I just have my LLM now within this between this LLM there will be an MCP protocol okay and with this particular MCP protocol now we are communicating with all these services. Okay. And the best part is that this entire services along with this communication with the protocol along with this communication with the protocol is basically handled by the service provider. I don't have to worry about it. Now this communication this this entire thing is basically managed right this is managed now by service provider. managed by service provider. So that basically means now whatever updates it comes right now this is my MCP protocol right this is my MCP protocol right now all I have to do is that LLMs where I have right my AI assistant here I have to just create an MCP client this MCP client will specifically use the MCP protocol and it can now communicate communicate with this services right it is just like an USBC cable Okay, it is just like an USB C cable. Now with the help of this USB C cable, you can probably connect it to your mobile phone, hard disk, external hard disk or anything as such. Now this is my service A, this can be my service B, this can be my service B. Now here whatever changes happens, this is basically managed by the service provider, right? I don't have to probably go ahead and update the code because the code that I usually write is just creating a MCP client and connecting and basically using this MCP protocol to connect this right here. no updation of the specific code will basically happen right now this is what is really important right this are basically called as or you can basically say that I'm using MCP protocol sorry here I will be using MCP protocol just say see this okay so here I'm basically using MCP protocol and at the end of the day here I have something called as MCP servers right this server it can be one server it can be multiple servers every server It can be connected to multiple services. It can be connected to one services. Right? Now here this entire server plus whatever information we are getting from the services is basically handled by the service provider. Right? Now what we are basically going to do in this tutorial and again the entire video has been already uploaded in my YouTube channel related to what is MCP. You can go ahead and probably refer this. Okay. Now what we will be doing with respect to practical applic uh application we will try to show you how to go ahead and create MCP server okay because many people had requested it with respect to this particular MCP server how do we go ahead and create the tools right tools or services what all all the specific services we'll try to show you with the help of Python SDK then I will also try to show you how to probably go ahead and create this MCP client which communicates with the MCP protocol to the MCP server, right? Which communicates with MCP protocol to the MCP server. So that part also I will try to show it to you. Okay, but I hope you got a very clear idea, right? Here I have MCP server. Here I have MCP protocol. This MCP client uses MCP protocol to communicate to the MCP server which in turn is communicating to various services. And LLM is very much smart enough because whatever tools is basically created there some information will be there. Okay, some information will be there uh like doc string and all llm will be able to understand based on a specific request which servers provider we really need to go ahead and communicate and there was one more very amazing thing about this is that I already uh I think I've covered this you know so how there's a communication basically happens okay so here here is the entire communication I have this uh host MCP client first of all always understand how the communication will specifically happen. Uh let me just see this. Yeah, here you go. So as soon as we get an input, first the host based on which client we have created will go ahead and hit the MCP servers and the MCP servers will give the information which all specific tools it has. Then along with the input and the tools, it goes to the LLM. Then the LLM decides which tools to use. It'll go back to the MCP client and then this MCP host specifically uses this particular tool. Okay. and we get that particular output over here and that output is entirely sent to the LM and final output is we get it over here. This everything I've explained it in my previous video. Okay. Now what I will do I will show you everything from scratch how you can go ahead. Okay. So here uh you'll be able to see that I'm there in my MCP course uh folder. So this is my new course folder. what I am actually going to do over here and uh from this particular course folder what I will be doing is that I will just go ahead and open my cursor if you want you can use cursor otherwise first of all I will use UV okay so UV in it I will create a folder let's say MCP crash course this will be basically my folder okay crash course okay now everything I'm going to basically do it from basics right so then I'll go to I'll close this. Okay. So here if you see my crash course is basically created and all the normal files like main.py project.2ml readme file everything is basically created right now I'll just go ahead and open this with cursor ID. I'm going to specifically use cursor ID. If you don't have you can probably go ahead and just download it uh quickly. Okay. So this is my cursor ID which has opened it. Okay. Uh let me do one thing. Let me see whether I have any extensions. Some kodium extension is there which will look much more better. Okay. So I'll just go ahead and write kodium kodium set color theme. This is discovering Python interpreters. So is this looking good? Uh yeah, I guess so. Uh let's see some other um things. I will probably go ahead and have a look onto this. Okay, I'll close this right now. I will open with cursor quickly and I'll open the extension. I'll go to Kodium. I think we had something like kodium uh for dark thing also I have right midnight coding dark midnight kodium kodium dark syntax set color theme I think it is not there let's install this okay I I really want I like dark okay so that is the reason I'm specifically using this but it's okay yeah this looks good perfect so this kind of situation will also be coming up Okay. Uh, okay. Sorry. Sorry. Sorry. Sorry. I'll just go ahead and set. Okay. Uh, we can also set this amaloid. This also looks good. Perfect. Now, this this looks good. Okay. I can just go ahead and open my main. py file. This is looking perfect. Okay. All these things are there. Now, uh, this is what is my terminal. Okay. So, now what I will do, I will just first of all show you how to create a server from scratch. So, let's say this is my server folder. Okay. Now inside this server folder um I will just go ahead and create some weather. py file. Let's say I want to create some services. Let's say if you ask me any question related to the weather alerts. This server this MCP server should be it's just like it is if I'm creating this server right it is simply you'll be able to see that I'll be creating this server along with the services. The server in short communicate with the services and give me the output right. So this is what I'm actually developing. So in server what I will do I will just go ahead and create my another file which is called as weather. py. Okay. Now inside this weather py my main aim is to probably go ahead and um create some weather services. And before this what I will do I will just go ahead and open my terminal. I will open my command prompt. I will be installing some of the libraries. Before that as usual I have to go ahead and create my environment variable. So I will write uv venv. Okay. Once I do this here you'll be able to see with the help of Python 3.13 my environment has basically got created. Now I'll activate this environment quickly. So this is what is my activation. So my MCP crash course activ uh environment is basically activated. Perfect. Till here it is very simple. So how do you activate it? By just using this UV venv and my venv has got activated. Now, uh, one thing that I'm actually going to do is that I will also be making sure to probably go ahead and add a library, right? So, let me just hide my face so that you will be able to see the screen. So, I'll write UV add and here I will just use MCP CLI. Okay, this MCP CLI will be important because uh we will we are going to basically use a specific library which is called as uh fast MCP which is provided by this this entire Python SDK. So if you see over here Python SDK inside this here you'll be able to see that I will be using this MCP CLI. Okay. So this is my dependencies because I'll be using their SDK to specifically implement each and everything. Okay. So now I will quickly go over here. Perfect. Now this is very very simple till here we have done this my uh everything is ready my fast MCP uh CLI is ready itself okay now here uh what I'm actually going to do inside my weatherp it's basically all the services that I'm actually going to create okay like in my weather what all services I'm basically creating right and this should be run as a separate server right so my MCP server is running and internally whatever services I'm specifically using that we are going to use it over here. Okay. So, quickly I will go ahead and import some libraries. So, let's say from typing import any uh import httx uh from MCP server.fast MCP import fast MCP. This fast MPCP is provided by the Python HDK. Now you can see some warnings is basically coming over here. So what I will do I will just go ahead and set up this. I will set up the entire path. So here you'll be able to see V and V is there. Script is there. Okay, I will just go ahead and click on Python.exe. So once this is done, I think this issue is solved. Now we are starting or initializing our fast MCP server by this. Okay, if you just write like this, MCP is equal to fast MCP weather. Okay, now since we need to probably create our own functionalities that will be able to probably access the weather alerts and anything. So weather alerts and all for that uh we have a very open API that is completely for free to use that is nothing but https api.weather.gov. Okay. And here uh there is a specific user agent that we really need to use. Okay. Now based on this two information that is this user agent and some application some accept token we will be able to access this API weather. Okay. So if I'm making this uh API weather access, I will definitely be using something called as HTTPX. Okay, because this is supported um uh using this sync client. Okay, so again this documentation is very simple to follow. Okay, so what I'm actually going to do with those two information, I'm just going to go ahead and create a request over here. Right. So here it shows I'm creating a function a sync def make news request URL with dictionary this. Okay, it's nothing but it is making a request to the news API with proper error handling. Okay, so this is my API. Here is my header. Okay, as I said, this user agent needs to come over here and I'm writing a sync with HTTPS dot a sync client. Okay, so a sync client if you see and a synchronous HTTP client with connection pooling redirects cookie persistence it can be shared between task. Okay, so it's just like you are making a request for a specific website, right? which is completely open open available right here you don't even have to set up the tokens and all right so here we are using client get URL headers with this and timeout is 30 seconds and here I get the response in the form of JSON that is what we are basically doing with respect to this particular make news request okay now once I get the response it is also necessary that I return this in such a format you know and that format will be applicable so here we define a specific format and this we are creating here you can write anything I Don't worry like what how you want to probably return your services whether you want to return in the form of JSON and all. So here what we are doing is that whatever response we specifically get we will pass that response over here and we are just going to return the values in this manner. Okay, just like a string. Okay, this is perfect. So here these are my two API calls like two uh sorry one API call over here and this is just my another functionality which is alerting uh the entire uh uh output that we are getting. Okay, now comes the most important thing. Okay, because here we are going to implement two different services. So in order to define services, right? I will be defining something like MCP.OLool. Okay. So if I just go ahead and write like this MCP. So MCP always the name will be MCP over here. If you write app over here, then the uh decorator will become at the rate app. Here I'm writing MCP over here. Right? So MCP dot tool. Okay. Then uh what we do? we basically create a function let's say one of the function is something called as get alerts. So again another API call wherein I'm saying that hey here u this is my tool this is my service okay get alerts here is my dock string and this is really important because of this dock string the LLM will be able to understand okay when a specific request comes how do we need to probably go ahead and do so here it shows get weather alerts for a US state okay and then here you have all these things make news request URL the same up if not data features and we are basically getting all the values over here by the using this format alert right and we are just combining all this particular information just to give the output okay so this is just one one functionality I want to use it okay so this is this is very simple so here you can write your own custom code to create your services here you can even write code to probably connect to your database and giving your output in the form of JSON whatever things you really want here you can specifically write it okay here you can provide your connection string for a database let's say MongoDB and all here you are making a request to the MongoDB getting the response that's it and whatever tools and services you require you just need to go ahead and add this decorator so this basically becomes my tool okay one tool is get alerts this get alerts is basically making a call to this particular function getting the response and displaying it over here then applying this format alerts and displaying it over here that's it it's returning that same thing okay now this is perfect till here I think it is not very difficult to understand. Now this is my server. Okay. Now how do I run my server? That is the question. Okay. How do I run my server? As I said you that if you see over here, right? If I show you over here, right? This LLM, this AI assistant, this uh MCP client, whatever we are talking about, this can be a host, right? And whenever I consider host, this is let's say this is my host. My host creates an MCP client, right? So here an MCP client is basically created. So MCP client is basically created. Now this host can be anything, right? This can be let's say I'll start with a basic way. It can be a streamllet app. It can be a streamllet app. It can be a fast API app. Right. Fast API app. So it can be any front end application over here. The other way is that I can go ahead and even make cursor ID as my host. Okay. I can even make cloudy desktop as my host. I can even make insp mp inspector as my host. Okay. Now this host from this host we can go ahead and create our MCP client. Okay, we can go ahead and create our MCP client and this MCP client will be communicating with my MCP server. I hope it makes sense, right? It it makes sense and all the specific services you can basically use it right. This can be my service A, service B, service C, service D. What service I have created? It's something called as get alerts. How do I create this? I just have to use that decorator if I'm using that MCP right so MCP dot tools I've done that okay now first of all we will try to see how do we go ahead with MCP inspector I will also show you with cursor ID I'll also show you with cursor cloudy desktop along with that I will also show you directly with uh terminals also how do I go ahead with terminal everything I will show you okay now let's go ahead back to my code Okay, now this is my server right now. I'll go ahead and open my terminal. So, first of all, I will show you how do you run this entire thing in your inspector. Okay, MCP inspector in what is MCP inspector? I will tell you. Okay, so first of all, I will go ahead and write UV run MCP dev. Okay, I just need to go ahead and write this command. Now, just go ahead and call your weather.py file. Okay. So once I write this you uv run mcpdev server/weather. py. Once you basically write this it is showing hey uh okay I have done the spelling mistake. It should be server. Okay server. Okay. Now once I execute this it'll ask you to probably install the MCP inspector. Okay. Starting inspector at this particular port. Okay. Now if I just go ahead and click this link. So here you'll be able to see that my insp mcp inspector is there. Now just by writing that command uv run mcp dev whatever your server file name was. Okay. Now this is what is my mcp inspector and it looks something like this. Okay I'll make it dark so that you will be able to see this. Okay. Perfect. So now inside this MCP inspector there is a first important thing that you really need to understand. It is something related to transport type. Okay. Here you have two option std IO and there is something called as SSE. Okay. Now I'll go back to the documentation. If you go somewhere here and I will just search for uh MCPK Python SDK. No worries. I will show over here. So there is one page which is called as transports over here. Right? So if you go to model contract protocol here you will be able to see transports. Now in order to understand about transports there are two built-in transport types standard input output. So HDD IO transport enables communication through standard input output stream. This is particularly useful for local integration and command line tools. Let's say if you are creating the server and if you're creating the client all at one end okay then basically in the same local machine right that time you can use this std IO communication protocol okay so transport types you can basically use okay there is also one more which is called as server send events which is also nothing but SSE SSE transport enables server toclient streaming with HTTP post request for client client to server communication. Now there may be scenarios when you go to production right you have to host the server separately you have to host the client separately and client and server needs to communicate with the help of HTTP post request right at that point of time we will use server send events. This is the basic difference. If you are doing it in your local try to use HTDIO but at the end of the day it is always necessary that both your server and the client will be independently hosted at that point of time you can use server send events and the best part about MCP inspector is that it provides you both this mechanism okay to check it out right so if you see if I click on hdio it uses this command uv and it is running this server file separately okay so I can either connect it like this. So if I go ahead and click on connect, uh I will go ahead and connect it like this. Okay, now it has got connected. Okay. Now there are two important things. One is prompt, one is tools, right? Prompts and resources. I will talk about it later stages. Okay. Because this I'll tell you prompts are basic prompts. Whatever you write in the form of prompts and for that you use something else. Okay, I will talk about it. I'll just show you in just some time. Now tools over here. If I just go ahead and click on list tools. So it shows that SS connection not established. The reason uh is I'll just go ahead and execute this once again and let me just go ahead and show you SSE. I will go ahead and connect it. Is your MCP server running? Uh my MCP server is not running. Just a second. Is my MCP server closed or not? Okay. So there was an error. Okay. Uh, terminate. Let me just run it once again. Perfect. UV run mcp dev server weather. py. Okay. Now it is listening over here. So I will just go ahead and reload it. Okay. Now I will just go ahead and select htdio. I will connect it. Now I'll go to tools and let me list the tools. Now as soon as I list my tools, actually the server got disconnected before. So that is the reason I'm getting that error. Now you know in my tools I have created something called as get alerts right. So if I go over here here you'll be able to see that I've used this decorator MCP dot tools. So that is the reason you're getting this dot get alerts right get alerts. So this get alerts is over here. Okay. If I select this it says that hey get weather alerts for a US state. You can provide two characters. Let's say I want to get the weather alerts for state CA. I'll run the tool. As soon as I run the tool, you will be able to see that I'm getting total success and I'm getting the entire information and this information only I'm getting it from my services. Right? This I have created it right here. If you see in the weather py I'm basically hitting this URL. Okay? and then getting I'm making a request to this news request and getting the response and displaying this entire response returning this entire value right and that is what I'm actually doing so in short this is your first MCP server interaction that you have already created a tool and you're doing this okay now let me show you one more uh very good example uh with respect to other things right so here there are also options like resources prompts so let me just go ahead and see some prompts Okay, let's see some prompts here. Uh prompt is written. There is also something like resources. Okay. Uh resources is also there. I will see it with respect to the SDK. Okay. Other SDKs are basically given over here. So I will just search for Python SDK. Okay. And then let me just go ahead and click on resource. Now here you can see I will just go ahead and create this resource. Okay. And this resource is nothing but app configuration here. There's all things. So let me just copy this. And let me open my file again. Okay. And this time instead of writing at the rate MCP2, I will just go ahead and copy this and I'll reload it. Okay. I will reload it. So let me just close this. Terminate it. Now here we are not doing anything here. It is basically reading an config app. Okay. Other than this, if you want to just go ahead and create some greetings uh commands or anything as such, you can also do that. Okay, I will also show you uh that can we just directly do this with some greeting message, you know, and then you'll understand why resources is specifically used. Okay. So, what we are basically going to do over here is that we are just going to go ahead and use our basic message with respect to uh the resources, right? So just just display a greeting message. Uh I want something like uh a greeting message and uh probably display that right. And that part also we can see in the documentation. Let's see. So here uh review code is there. MCP tool is there. The documentation is the best place to probably see many things. Okay. So let's say this is one of my resource. Okay. See here you'll be able to see some examples where what does resource basically means? what the tools basically means. Okay, if I talk about tools, it it uh tool let LM take action through your servers. Unlike resources, tools are accepted to perform computation and side effects. So tools whenever you have kind of computation. Let's say I want to convert USD to um euro INR or INR to USD. I can probably create a tool and write all my functionalities. Resources are how you expose data to LLMs. They are similar to get endpoints in REST API. So I will just go ahead and create this. Okay. So for creating this what I will do I'll just copy this code that we saw like a echo message. Okay. With MCP resource. Okay. You can also do this. We can directly read a config app file. Okay. And we are just going to resource this. Okay. Now what I'll do I will again run this uvun mcpdev server/weather. py. This way we are basically running things in our mcp inspector. Okay. I will go ahead and connect this. Now you see in the resources if I click on resources there are two things one is uh method resources list and here you can see initialize okay uh if I list templates so here you can see there is something called as echo resource if I go ahead and click this and I enter my message let's say kish okay I will read the resource so here you can see echorish so this is just like an get get uh uh endpoint in chart right that is where resources is specifically used similarly if I go ahead and show you with respect to prompts prompts are reusable templates that help LLM interact with your server effectively let's say if you have any kind of prompt messages that you want to put please review this code right you can add that kind of code over there and along with the functionalities you can put it over there but again our main aim is basically to create the server side right that we are basically doing it okay Now let me go back again to the list tools get alerts. So here I will just go ahead and write CA NY whatever tool you basically whatever country state you want and you get the response. Similarly you can create any number of functionalities as you go ahead. Okay. So this is with respect to MCP inspector. Now the question arises kish um how do we probably proceed and do this uh cloud integration? How do we go ahead with all the other things at shut? Right. That is what you really need to see. So I will just go back to my file. Okay, weather. py. Now I will close this and I will show you how we can also run this in our in our MCP cloud sorry in our cloudy desktop and all. Okay. Now for doing that in the cloudy desktop uh we can directly install this. Okay. Okay. So here I will write uv run. Okay. MCP install. And here I will just go ahead and write my server/ whatever file I have weather. py. Okay. So once I execute this here you'll be able to see addit weather to the cloudy config. Okay. Automatically uv run mcp install server/weather. py. Okay. Now I will just open my cloudy. So here is my cloudy. And with respect to the cloudy I will just go ahead and see how many tools are here. So here you can see get forecast is available. Okay, get forecast is available over here. So if I just go ahead and ask, hey, what is the weather alerts in CA? Okay, I'll just go ahead and ask this question and I'll execute this. You'll be able to see that that you are basically checking this. I'll I'll check the current weather alerts for California. And here you can see all the information basically coming up. So that basically means the MCP server that we have created right it is now communicating with our cloudy desktop also. Now you may be thinking kish where did that configuration get added. So for that you go to file go to settings and here you'll be able to see developer right and here you can see I have added many many different servers tuck go searchnb uh weather right. So weather is my uh name right? uh I will just go ahead and click on edit config and here I will just edit with the notepad okay now here you'll be able to see that there is something called as weather so this is basically added right see all the other things are basically added over here demo Airbnb mcp servers dug go search each and everything so this kind of configuration gets added and you know this right this is my path of my weatherp file right so what it is doing based on this particular configuration the cloudy desktop knows We need to run this command uv run internally with mcpa cli mcp we have to use this weatherp and probably get all that services information that we have right I will copy this because I will be requiring this again okay and then I will go back over here so this is with respect to the cloudy desktop so we we showed you two ways okay one is this one is that okay directly to communicate it in the cursor I can go to preference references. I'll go to cursor settings. Now, inside my cursor settings, there is an option of MCP. And here I will just go ahead and edit this. Okay. And let me just update it over here. So, this is basically my weather. And here you can see that I have updated it, right? And this is with the same path that I have weather. py whether it will work or not. So, let's try this out. Okay. So here I will say hey uh provide me the weather alerts of CA. Okay. Now if I just go ahead and click on start I should be able to get the entire response. Okay. So let's see whether I'll help you with the weather alerts California using this. I'll run the tool and here I'm getting the information. So I've showed you through three different ways right? One is from the cloudy desktop. One is directly from the cursor. In the cursor, all you have to do is that whatever JSON you have actually copied it, just go to file, click on preferences, go to cursor settings, click on MCP and edit it. Right? Once you edit it, automatically your MCP.json file will come. And here you can see that I put that entire over here. Right? And through this you will be able to do this. Okay? And here you are able to get the entire response. Now this is another way. Okay, let me show you one more way. Now the thing is that I want to integrate it directly from my command line. Can I do that? I want to probably see this. This server probably should be running in some configuration. Uh I should provide this information in some configuration. It should directly run it. Okay. So for that I will go ahead and create one of my configuration. The configuration name will be weather.json. Okay. Now inside this weather.json JSON the same configuration I'll give it over here. Okay. So if you see with respect to my cloudy this is my configuration. I'll copy this. I'll paste it over here. On top of it uh what I will do I'll give MCP service because that is what the configuration wants. Okay. Um I just need to go ahead and write something like this. MCP MCP servers MCP servers. So please make sure to name it correctly because this is important. Okay. And I will close it over here. One bracket, two brackets. Okay. So I think this should work. Okay. One bracket over here. Let me close this. Okay. One bracket over here. And this is the one bracket. Okay. Okay, sorry. So this is what it is. I'm basically putting the entire configuration over here itself. Okay, and this way I'm basically saying that hey this is my configuration. This is my services details. Now can you run it from your local py file okay and probably get this information of the server and do the entire in entire things. Right. So for this uh inside the server what I will do I will just go ahead and create one client. py client py now this client should basically interact with that and here what I'm actually going to do is that I'm also going to show you how we can do the llm integration because at the end of the day from this weather py I need to probably get all the JSON right and then we should probably do the further things okay so I'll go to my client py and I will be requiring one more library so for this we are going to use something called as mcpus okay so mcpus is nothing But it is one open-source way to connect any LLMs to MCP server and build custom agents that helps tool access without using closed sources or application clients. Okay. So you know that I have created my server. Now what I will do I will quickly go ahead and write UV add uh what is the library name? Let's see uv add MCPUs. So I will just go ahead and write this MCPUs. I'm going to use this. I'll paste it over here. Okay. And this ue use will basically get added. Okay. Now I will quickly go ahead and import all the necessary libraries. And before that let me just go ahead and even create file because I want to use some kind of LLMs also right. So for this I will be using grock API key. So here is my gro API key. So here is my entire gro API key itself. Okay. Now with respect to this gro API key, uh I will also go ahead and import since I'm using grock, I'll be writing uv add lang chain / crock because I will be requiring this. So here you can see how quickly it is basically happening with the help of uv right that is superb right now what I will do I will import all this libraries inside my client. py because this will be interacting with this weather. by now. Okay. So from asyn import asyno um then lang chin grock from chat gro then you have mcp use mcp agent and client. Okay. Now what we are basically going to do uh over here is that uh we are just going to go ahead and create my chat. py file which will be communicating with my entire JSON configuration and it should basically give me the output. Right. So quickly I will explain you this specific code. See guys uh the code is quite easy not that difficult. If you know basic Python you should be able to understand it right. So here you can see uh my weather.json is basically over here. I'm just going to go ahead and load my envi key. I'm giving my configuration file. Then this client will be reading. So MCP client what we have imported over here. This will basically be reading my config file. Right. Server/weather.g. JSON and then my LLM will be equal to chat grock model with this particular model and then finally we create our MCP agent where we require LLM client and uh all the other features over here this way we'll be able to interact it over here right so let me just show you after interacting it how it looks like okay so here you'll be able to get an idea so I'll write uvun server/client py okay not Finally this any configuration any inbuilt configuration also you have you should be able to use it directly through this okay so I've written uvun server/client py so the first time it'll take time so I can say hey uh provide me the weather alerts weather alerts for c okay directly from the command line I'm doing this okay later on you can convert this into a streaml app and all see here you're getting all the response Right? 4:00 a.m. to 8:00 a.m. all these information. Take steps now to perfect tender plants from the cold. All these things are there. Right? So this is how the interaction is this time. Now if I just go ahead and show you this particular code. What all things we basically did? We created MCP agent. We created MCP client. This MCP client will be reading from this particular config file that is weather.json. It knows all the information about the server. Okay. So once it gets connected okay then we are loading in the LLM model. This client is going to provide the context to this LLM which is acting in the form of agent. Okay. So almost each and everything we covered over here. I think this will be a great start for you all to probably go ahead and discuss more about this. Yes, in the upcoming future videos I will also be showing you how uh you can probably go ahead and uh do this uh with the help of uh Streamlit and all. Okay. So guys, till now we have developed almost many things over here and now it's time that we should also see that how we can dockerize this entire example also. Okay. So here what I will do do is that I'll create another folder. Okay. I'll write server server or I'll just say MCP server. Okay. MCP server. And with respect to this particular MCP server uh here I'm going to go ahead and create my server. py file let's say. Okay. And based on this two protocols which I have not mentioned yet. Okay. So we can also go ahead and create this server. py. Now inside this server py I will use the same weather alerts code. Okay. Uh so here in this weather alerts code what I will be doing is that there are some more parameters that I really want to give uh before I go ahead you know one uh is that I am saying that hey it should run in this host and in this port 8050. Okay. And further if you see that if I'm running this with the help of transport as SSC it should use MCP.run transport MC SSE otherwise it should basically run H stdiodio. Okay. So both these specific options I' actually given now the next thing that I really want to add is my client SSE. So let's say that if I'm running with SSE transport protocol I have to read it with SSE transport protocol. So if you probably go ahead and see over here server server events right server sent events and there is also like standard input output. If you're using standard input output this is the code that we really need to go ahead and write in order to communicate with the server. If you're using server sent events uh this is the code that we really need to communicate. And here you can see 8,000 port is given over here. Whatever port we have given we have to write that. Okay. So if I go ahead and write this over here I'll just go ahead and create one client. So client client dash SSE py. Okay. Now with respect to this client uh ss. py I will just go ahead and uh show you the code what we can write and I'll explain you the code. Okay. So here you can see a sync io nest async io. Remember to install both these libraries by writing uv add this particular library. Then you have this client session. Whenever we use SSE transport, we have to also use from MCP.clients secs client. And this SSE client will be communicating with this particular URL. And this URL is where your server is basically running. Okay. So your server, see over here, make sure the server is running before running this script. The server is configured to use SSA transport and the server is listening on port 8050. To run the server, uv server.py. So let me just go ahead and quickly run this. Okay. So I will say hey um uv run mcp server dot / server py. Okay. So once I see this uh okay some some error I think error is there. I will delete this. Okay. Okay. Same port I think same port I was running something else. Okay. Now if I run this now you can see that this server is running. Okay. Now I'll go to my command prompt. And now I will just go ahead and run this. So once you create the client SSC. So all you have to do is that since your uh since your server is already running, you just need to go ahead and write uv run uv um mcp server/client ss e py. Okay. So once you write this already uh it'll just go ahead and give you the output with respect to this particular weather alerts. Okay. So this is with respect to sec. Now you know that many people will also say hey Krisha if it is if you're using HTD IO then what is the code that we need to write? Okay so for that also I'll give you this entire skeleton so that you can refer it. I don't want to make this uh entire video very very long so that it'll become very difficult for you to focus and concentrate. Okay. So what I will do I'll write hdio. py. So this will basically be my file for it. Okay. And here we use this particular code where we are running this python server py. That's it. Okay. and remaining everything will be almost same here. This is just a code to probably connect to all the tools that we have and probably provide you the information. But the most important thing will be that how do we go ahead and create our docker file. Okay. So for this first of all I will just go ahead and create my requirements.txt. Okay. Now you know in requirement.txt what all things you basically require. I require mcpi right. So quickly I will just go ahead and write mcpi. Okay. Now with respect to this MCP CLI I will just go ahead and create my docker file. Now so for this just go ahead and write docker file. Okay. Now with respect to the docker file I have already created this. So you should be able to refer it. Okay quickly. So what in in the case of docker what I'm actually doing I'm saying hey uh from python 3.11 uh slim work/ app run pip install. And here you'll be able to see we are exposing this 8050 all those things. Okay. And let me do one thing. Let me also expose some other some other files and all some other port that will be helpful for you. And I'll run this also. So let's say instead of 8050 I will use in this server something like 8,000. So that should not give you an issue. All right. So this will be 8,000. And let me just go ahead and implement this and write uv run mcp server slash server. py. Okay. Once this is running, let me also run the client so that you should be able to see what kind of output you're getting. And for the client SSC, I'll make this as 8,000. You have to please make that changed. Okay. Change. Okay. So now I'll write uv run uh mcp server/client ssse.p py. Okay. Uh so here you can see I'm getting the entire response argument available tool get alerts everything is over here and I'm able to get it. Okay. This is the beautiful thing. Now I will close this. This is closed and I'll also close this. Okay. Now it's time that we I try to show you that how we can create this in the form of docker file. In docker file I'll not expose 80 but instead 8,000. Okay. Uh we'll do this uh with respect to the 8,000 itself. I'll go ahead and create a docker file. So here what we are doing this is my working directory. I'm saying hey go ahead and pip install uv copy everything that is present in requirement.txt. Then you run uvnv to create the uh to create to create the specific uh virtual environment. Then pip install minus r requirement.xt copy server. py then client ssc.py whatever files you basically required. Then you expose 8,000 over there. And then you can probably run this particular command uv server round uvunner server. py right and after doing this all you have to do is that just go ahead and build this docker. Okay. So in order to build it, I will just go ahead and open my command prompt quickly. I will write hey uh docker build minus t mcp / server. Okay. So once I write this it will start building. Okay. Build x requires one argument. Okay. Okay. I'll just write dot. So here you can see uh fail to solve docker file open docker no such file directory. Why it is giving this m okay let me just go ahead and write this um build build okay I have to probably go ahead and in that particular folder so I'll write cd mcp server okay and then I will go ahead and probably create this docker build mtb server minus t so automatically the build process will actually happen right once the build happens uh all you have to do is that just go ahead and run this command to probably push Let's say the port is nothing but 8,000. You can also use this 8,000. Right? So this was all the information. Uh you can just directly go ahead and run this MCP server over there. Automatically it'll start running and you can see that in your desktop docker desktop. Right? So I hope uh you like this particular video. This was a kind of crash course where I showed you almost each and everything. You can probably get out all this information from the GitHub link uh I will be giving in the description. But at the end of the day, it's more about understanding how MCP server works, what kind of code you specifically need to write and many things as such. So yes, this was it from my side. I will see you in the next video. Have a great day. Thank you, Wall. Take care. Bye-bye.\n"
     ]
    }
   ],
   "source": [
    "response = loader.load()\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow\n",
    "\n",
    "input (Yt video url) => load the transcript (from Yt) => assign a topic to it => assign a specialist of that topic => then give summary of the transcirpt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "#\n",
    "\n",
    "class YTState(TypedDict):\n",
    "    url : str\n",
    "    transcript : str\n",
    "    author : str\n",
    "    author_info : str\n",
    "    topic : str\n",
    "    summary : str\n",
    "\n",
    "class YTSpecialist(BaseModel):\n",
    "    specialist : str\n",
    "    specialist_bio : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchTranscript(state : YTState):\n",
    "    loader = YoutubeLoader.from_youtube_url(\n",
    "         state[\"url\"], add_video_info=False\n",
    "    )\n",
    "    response = loader.load()\n",
    "    return {\"transcript\": response[0].page_content}\n",
    "\n",
    "\n",
    "def assign_topic(state : YTState):\n",
    "    prompt = f\"\"\"\n",
    "        You are an AI assistant that identifies the main topic of a transcript.\n",
    "        Please read the following excerpt from a transcript and determine the **primary topic** being discussed. Be concise and return **only the topic** as a short phrase or sentence, without any additional explanation or formatting.\n",
    "        Transcript:{state['transcript'][:2000]}  \n",
    "        In output i want only the topic \n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"topic\": response.content}\n",
    "\n",
    "def assign_specialist(state : YTState):\n",
    "\n",
    "    prompt = f\"\"\" You are an AI assistant responsible for assigning the most appropriate real-world expert or well-known specialist to summarize content on a specific topic.\n",
    "\n",
    "    The expert must be:\n",
    "    - A real, existing person (no fictional or AI-generated names)\n",
    "    - Recognized in their field with proven credentials or contributions\n",
    "    - Relevant to the given topic, either through academic, industry, or public work\n",
    "\n",
    "    Topic: \"{state[\"topic\"]}\"\n",
    "    Return me :\n",
    "    specialist : Name of person/expert\n",
    "    specialist_bio : A short background on the expert  who they are and why they're relevant to this topic.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(YTSpecialist)\n",
    "    response = structured_llm.invoke(prompt)\n",
    "\n",
    "    # print(response.specialist)\n",
    "    # print(response.specialist_bio)\n",
    "\n",
    "    return {\n",
    "        \"author\" : response.specialist , \n",
    "        \"author_info\" : response.specialist_bio\n",
    "    }\n",
    "\n",
    "def generate_summary(state):\n",
    "\n",
    "    prompt = f\"\"\" \n",
    "        \n",
    "       You are {state[\"author\"]}, widely recognized for: {state[\"author_info\"]}.\n",
    "       \n",
    "       Your task is to generate a **detailed, entailed summary** of the following YouTube transcript. The summary should sound like it's written entirely in your **unique voice**reflecting your tone, perspective, and domain expertise.\n",
    "\n",
    "        Instructions:\n",
    "        - Write a **detailed summary** that thoroughly captures all key points, arguments, examples, and conclusions.\n",
    "        - Every statement must be **entailed**fully supported by the content in the transcript (no hallucinations or external assumptions).\n",
    "        - Reflect the **structure and progression** of ideas in the transcript.\n",
    "        - Use a style and vocabulary that **aligns with how you speak or write** to your audience (e.g., blog, podcast, or talk).\n",
    "        - Organize the summary in a logical and readable way (bullet points or short paragraphs if needed).\n",
    "\n",
    "         **Topic**:  \n",
    "        {state[\"topic\"]}\n",
    "\n",
    "         **Transcript**:  \n",
    "        {state[\"transcript\"][:3000]}\n",
    "\n",
    "        Only return the detailed summary, nothing else.\n",
    "         \"\"\"\n",
    "    \n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {\n",
    "        \"topic\" : state[\"topic\"],\n",
    "        \"author\" : state[\"author\"],\n",
    "        \"author_info\" : state[\"author_info\"],\n",
    "        \"summary\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Gen-ai',\n",
       " 'author': 'Alfred',\n",
       " 'author_info': 'Rsearcher and gen-ai expert',\n",
       " 'summary': 'My esteemed colleagues, \\n\\n\"Gen-ai is the new era\"  a bold statement, indeed. And while I wouldn\\'t go so far as to declare a complete paradigm shift, it\\'s undeniable that generative artificial intelligence is rapidly reshaping our landscape. We stand on the precipice of a fascinating evolution, where machines can not only process information but also create it   text, images, even code.  This opens up a world of possibilities, but also presents us with significant challenges. We must proceed with both caution and enthusiasm, ensuring responsible development and deployment of this powerful technology. The future is being written, and it\\'s incumbent upon us to ensure it\\'s a future we can all be proud of. \\n\\n\\n\\n\\n\\n'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_summary(\n",
    "    {   \"topic\" : \"Gen-ai\",\n",
    "        \"transcript\" : \"Gen-ai is the new era\",\n",
    "        \"author\" : \"Alfred\" , \n",
    "        \"author_info\" : \"Rsearcher and gen-ai expert\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph , START , END\n",
    "\n",
    "graph_builder = StateGraph(YTState)\n",
    "\n",
    "graph_builder.add_node( \"fetchTranscript\" , fetchTranscript )\n",
    "graph_builder.add_node( \"Assigning_Topic\" , assign_topic )\n",
    "graph_builder.add_node( \"Assigning_Specialist\" , assign_specialist )\n",
    "graph_builder.add_node( \"Summary Generator\",generate_summary)\n",
    "\n",
    "graph_builder.add_edge(START , \"fetchTranscript\")\n",
    "graph_builder.add_edge(\"fetchTranscript\" , \"Assigning_Topic\")\n",
    "graph_builder.add_edge(\"Assigning_Topic\" ,  \"Assigning_Specialist\")\n",
    "graph_builder.add_edge( \"Assigning_Specialist\" , \"Summary Generator\" )\n",
    "graph_builder.add_edge( \"Summary Generator\" , END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image , display\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['url', 'transcript', 'author', 'author_info', 'topic', 'summary'])\n",
      "The Model Context Protocol (MCP), an initiative spearheaded by Anthropic, is gaining traction as a crucial tool for developing agentic AI. Essentially, MCP provides a standardized way to inject and manage context within large language models (LLMs), enabling them to perform more sophisticated and contextually aware tasks. \n",
      "\n",
      "This growing ecosystem allows developers to build and integrate MCP servers, acting as intermediaries between LLMs and external applications or services. Imagine this: an LLM-powered chatbot that can seamlessly access your calendar, email, or even specialized databases to provide truly helpful and personalized responses. That's the power of MCP.\n",
      "\n",
      "There are various implementations and integrations available, including open-source tools and cloud-based solutions.  As research progresses, we'll likely see even more innovative applications of MCP, pushing the boundaries of what's possible with AI. \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"https://www.youtube.com/watch?v=MDBG2MOp4Go\"\n",
    "response = graph.invoke({\"url\" : input})\n",
    "print(response.keys())\n",
    "print(response[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashjain/Desktop/AgentVerse/AgentVerse/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/akashjain/Desktop/AgentVerse/Backend/src/backend')\n",
    "\n",
    "from LLMs.groqllm  import GroqLLM\n",
    "from graph.Yt_graph import YT_GRAPH_Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase = \"YT_AI\"\n",
    "obj_llm = GroqLLM()\n",
    "model = obj_llm.get_llm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = YT_GRAPH_Builder(model)\n",
    "graph = graph_builder.setup_graph(usecase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"url\" : \"https://www.youtube.com/watch?v=_3ezSpJw2E8\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['url', 'transcript', 'author', 'author_info', 'topic', 'summary'])\n"
     ]
    }
   ],
   "source": [
    "print(response.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alright, creators, let's break down what's going on with this transcript situation.  \\n\\nSo, the video in question doesn't have a transcript in English, which is what you initially requested.  The system tells you that no transcripts were found for English. \\n\\nBut hold on! It gets interesting. The video does have a transcript, but it's in Hindi (generated automatically), and it's marked as translatable. \\n\\nNow, here's where things get really cool. YouTube lists a bunch of other languages where a transcript *could* be created for this video.  It's like a treasure map of potential translations!  Think of it as an opportunity to make your content accessible to a wider audience.  \\n\\n\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"summary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transcript availability for a YouTube video \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"topic\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=_3ezSpJw2E8! This is most likely caused by:\\n\\nNo transcripts were found for any of the requested language codes: [\\'en\\']\\n\\nFor this video (_3ezSpJw2E8) transcripts are available in the following languages:\\n\\n(MANUALLY CREATED)\\nNone\\n\\n(GENERATED)\\n - hi (\"Hindi (auto-generated)\")[TRANSLATABLE]\\n\\n(TRANSLATION LANGUAGES)\\n - ab (\"Abkhazian\")\\n - aa (\"Afar\")\\n - af (\"Afrikaans\")\\n - ak (\"Akan\")\\n - sq (\"Albanian\")\\n - am (\"Amharic\")\\n - ar (\"Arabic\")\\n - hy (\"Armenian\")\\n - as (\"Assamese\")\\n - ay (\"Aymara\")\\n - az (\"Azerbaijani\")\\n - bn (\"Bangla\")\\n - ba (\"Bashkir\")\\n - eu (\"Basque\")\\n - be (\"Belarusian\")\\n - bho (\"Bhojpuri\")\\n - bs (\"Bosnian\")\\n - br (\"Breton\")\\n - bg (\"Bulgarian\")\\n - my (\"Burmese\")\\n - ca (\"Catalan\")\\n - ceb (\"Cebuano\")\\n - zh-Hans (\"Chinese (Simplified)\")\\n - zh-Hant (\"Chinese (Traditional)\")\\n - co (\"Corsican\")\\n - hr (\"Croatian\")\\n - cs (\"Czech\")\\n - da (\"Danish\")\\n - dv (\"Divehi\")\\n - nl (\"Dutch\")\\n - dz (\"Dzongkha\")\\n - en (\"English\")\\n - eo (\"Esperanto\")\\n - et (\"Estonian\")\\n - ee (\"Ewe\")\\n - fo (\"Faroese\")\\n - fj (\"Fijian\")\\n - fil (\"Filipino\")\\n - fi (\"Finnish\")\\n - fr (\"French\")\\n - gaa (\"Ga\")\\n - gl (\"Galician\")\\n - lg (\"Ganda\")\\n - ka (\"Georgian\")\\n - de (\"German\")\\n - el (\"Greek\")\\n - gn (\"Guarani\")\\n - gu (\"Gujarati\")\\n - ht (\"Haitian Creole\")\\n - ha (\"Hausa\")\\n - haw (\"Hawaiian\")\\n - iw (\"Hebrew\")\\n - hi (\"Hindi\")\\n - hmn (\"Hmong\")\\n - hu (\"Hungarian\")\\n - is (\"Icelandic\")\\n - ig (\"Igbo\")\\n - id (\"Indonesian\")\\n - iu (\"Inuktitut\")\\n - ga (\"Irish\")\\n - it (\"Italian\")\\n - ja (\"Japanese\")\\n - jv (\"Javanese\")\\n - kl (\"Kalaallisut\")\\n - kn (\"Kannada\")\\n - kk (\"Kazakh\")\\n - kha (\"Khasi\")\\n - km (\"Khmer\")\\n - rw (\"Kinyarwanda\")\\n - ko (\"Korean\")\\n - kri (\"Krio\")\\n - ku (\"Kurdish\")\\n - ky (\"Kyrgyz\")\\n - lo (\"Lao\")\\n - la (\"Latin\")\\n - lv (\"Latvian\")\\n - ln (\"Lingala\")\\n - lt (\"Lithuanian\")\\n - lua (\"Luba-Lulua\")\\n - luo (\"Luo\")\\n - lb (\"Luxembourgish\")\\n - mk (\"Macedonian\")\\n - mg (\"Malagasy\")\\n - ms (\"Malay\")\\n - ml (\"Malayalam\")\\n - mt (\"Maltese\")\\n - gv (\"Manx\")\\n - mi (\"Mori\")\\n - mr (\"Marathi\")\\n - mn (\"Mongolian\")\\n - mfe (\"Morisyen\")\\n - ne (\"Nepali\")\\n - new (\"Newari\")\\n - nso (\"Northern Sotho\")\\n - no (\"Norwegian\")\\n - ny (\"Nyanja\")\\n - oc (\"Occitan\")\\n - or (\"Odia\")\\n - om (\"Oromo\")\\n - os (\"Ossetic\")\\n - pam (\"Pampanga\")\\n - ps (\"Pashto\")\\n - fa (\"Persian\")\\n - pl (\"Polish\")\\n - pt (\"Portuguese\")\\n - pt-PT (\"Portuguese (Portugal)\")\\n - pa (\"Punjabi\")\\n - qu (\"Quechua\")\\n - ro (\"Romanian\")\\n - rn (\"Rundi\")\\n - ru (\"Russian\")\\n - sm (\"Samoan\")\\n - sg (\"Sango\")\\n - sa (\"Sanskrit\")\\n - gd (\"Scottish Gaelic\")\\n - sr (\"Serbian\")\\n - crs (\"Seselwa Creole French\")\\n - sn (\"Shona\")\\n - sd (\"Sindhi\")\\n - si (\"Sinhala\")\\n - sk (\"Slovak\")\\n - sl (\"Slovenian\")\\n - so (\"Somali\")\\n - st (\"Southern Sotho\")\\n - es (\"Spanish\")\\n - su (\"Sundanese\")\\n - sw (\"Swahili\")\\n - ss (\"Swati\")\\n - sv (\"Swedish\")\\n - tg (\"Tajik\")\\n - ta (\"Tamil\")\\n - tt (\"Tatar\")\\n - te (\"Telugu\")\\n - th (\"Thai\")\\n - bo (\"Tibetan\")\\n - ti (\"Tigrinya\")\\n - to (\"Tongan\")\\n - ts (\"Tsonga\")\\n - tn (\"Tswana\")\\n - tum (\"Tumbuka\")\\n - tr (\"Turkish\")\\n - tk (\"Turkmen\")\\n - uk (\"Ukrainian\")\\n - ur (\"Urdu\")\\n - ug (\"Uyghur\")\\n - uz (\"Uzbek\")\\n - ve (\"Venda\")\\n - vi (\"Vietnamese\")\\n - war (\"Waray\")\\n - cy (\"Welsh\")\\n - fy (\"Western Frisian\")\\n - wo (\"Wolof\")\\n - xh (\"Xhosa\")\\n - yi (\"Yiddish\")\\n - yo (\"Yoruba\")\\n - zu (\"Zulu\")\\n\\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"transcript\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgentVerse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
